{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "natural_language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMb2ySd8X/aIZenf8wjeexe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvjpavankumar/mycode/blob/main/natural_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk_yOG_yzdO1"
      },
      "source": [
        "Natural Language Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eKx4u5yza17"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "212Bbid2zykL"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArZ7dmBkz4ew"
      },
      "source": [
        "#first insert the string to a variable\n",
        "string = \"HYDERABAD\"\n",
        "#get first alphabet with index\n",
        "print(string[0])\n",
        "\n",
        "#printing multiple alphabets\n",
        "print(string[2], string[5])\n",
        "\n",
        "#for getting alphabet with negative indexing\n",
        "print(string[-4])\n",
        "\n",
        "#characters with slicing\n",
        "print(string[0:2])\n",
        "#output: GU\n",
        "print(string[1:4])\n",
        "#output: URU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mweGsOvp1Ekh"
      },
      "source": [
        "strip() function -  This function removes character in the starting and from the end, but it cannot remove character in the middle. If we don’t specify a removing character, then it will remove spaces by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KpMAIk00uLp"
      },
      "source": [
        "#A sentence and the removing character from the sentence\n",
        "sentence = \"****Hello World! I am Pavan Kumar****\"\n",
        "removing_character = \"*\"\n",
        "#using strip function to remove star(*)\n",
        "sentence.strip(removing_character)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe0oBVLc1VMV"
      },
      "source": [
        "Join Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWmyEKx51WN8"
      },
      "source": [
        "str1 = \"Happy\"\n",
        "str2 = \"Learning\"\n",
        "\" NLP \".join([str1, str2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsGm-frb1xVK"
      },
      "source": [
        "Tokenization: When a sentence breakup into small individual words, these pieces of words are known as tokens, and the process is known as tokenization.\n",
        "The sentence breakup in prefix, infix, suffix, and exception. For tokenization, we will use the spacy library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBfKXcnM1wzq"
      },
      "source": [
        "#import library\n",
        "import spacy\n",
        "#Loading spacy english library\n",
        "load_en = spacy.load('en_core_web_sm')\n",
        "#take an example of string\n",
        "example_string = \"I'm going to meet Telugu Famous Action Mr. Chiranjeevi\"\n",
        "#load string to library \n",
        "words = load_en(example_string)\n",
        "#getting tokens pieces with for loop\n",
        "for tokens in words:\n",
        "    print(tokens.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD86n04G2bzD"
      },
      "source": [
        "Stemming : is a process in which words are reduced to their root meaning. We have 2 types of stemmer: Porter Stemmer and Snowball Stemmer. Spacy doesn’t include a stemmer, so we have to use the NLTK library for the stemming process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWk00p-s2dhO"
      },
      "source": [
        "Below is the Port Stemmer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffOel9NC2hEz"
      },
      "source": [
        "#import nltk library\n",
        "import nltk\n",
        "#import porter stemmer from nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "pot_stem = PorterStemmer()\n",
        "#random words to test porter stemmer\n",
        "words = ['happy', 'happier', 'happiest', 'happiness', 'breathing', 'fairly']\n",
        "for word in words:\n",
        "    print(word + '----->' + pot_stem.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id0yUMMb2oH5"
      },
      "source": [
        "As we see above, the words are reduced to its stem word, but one thing is noticed that the porter stemmer is not giving many good results. So, that's why the Snowball stemmer is used for a more improved method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHnpS0DP2sz0"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snow_stem = SnowballStemmer(language='english')\n",
        "for word in words:\n",
        "    print(word + '----->' + snow_stem.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pblzPNl20d8"
      },
      "source": [
        "Lemmatization: is better than stemming and informative to find beyond the word to its stem also determine part of speech around a word. That’s why spacy has lemmatization, not stemming. So we will do lemmatization with spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqj3hRke23js"
      },
      "source": [
        "# import spacy\n",
        "# load_en = spacy.load('en_core_web_sm')\n",
        "# take an example of string\n",
        "example_string = load_en(u\"I'm happy in this happiest place with all happiness. It feels how happier we are\")\n",
        "for lem_word in example_string:\n",
        "    print(lem_word.text, '\\t', lem_word.pos_, '\\t', lem_word.lemma, '\\t', lem_word.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBa-OMHx3Ei1"
      },
      "source": [
        "In the above code of lemmatization, the description of words giving all information. The part of speech of each word and the number in the output is a specific lemma in an English language library. We can observe that happiest to happy and happier to happy giving good results than stemming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITlv2-hy3Ht4"
      },
      "source": [
        "Stop words: are used to filter some words which are repeat often and not giving information about the text. In Spacy, there is a built-in list of some stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNLYVYiI3GMR"
      },
      "source": [
        "# import spacy\n",
        "# load_en = spacy.load('en_core_web_sm')\n",
        "print(load_en.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHBoF4qZ3fH-"
      },
      "source": [
        "Part of Speech (POS): is a process to get information about the text and words as tokens, or we can say grammatical information of words. Deep information is very much important for natural language processing. There are two types of tags. For the noun, verb coarse tags are used, and for a plural noun, past tense type, we used fine-grained tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2XD8dLM3jzh"
      },
      "source": [
        "# import spacy\n",
        "# load_en = spacy.load('en_core_web_sm')\n",
        "str1 = load_en(u\"This laptop belongs to Amit Chauhan\")\n",
        "#pos_ tag operation \n",
        "print(str1[1].pos_)\n",
        " \n",
        "#to know fine grained information\n",
        "print(str1[1].tag_)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH6OXLkG30xT"
      },
      "source": [
        "So the coarse tag is a NOUN, and the fine grain tag is NN, so it says that this noun is singular. Let's get to know what is POS count with spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzT3zY3l330g"
      },
      "source": [
        "pos_count = str1.count_by(spacy.attrs.POS)\n",
        "pos_count\n",
        "str1.vocab[90].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgB1Aear38VH"
      },
      "source": [
        "DET means that the 90 number belongs to determiner and the value 1 belongs to it is that this DET repeated one time in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDv2pz8w4Eq-"
      },
      "source": [
        "**Named entity recognition (NER)**: is very useful to identify and give a tag entity to the text, whether it is in raw form or in an unstructured form. Sometimes readers don't know the type of entity of the text so, NER helps to tag them and give meaning to the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpWRbUFf4NQi"
      },
      "source": [
        "# import spacy\n",
        "# load_en = spacy.load('en_core_web_sm')\n",
        "# lets label the entity in the text file\n",
        "file = load_en(u\" I am living in India, Studying in IIT\")\n",
        "doc = file\n",
        "if doc.ents:\n",
        "    for ner in doc.ents:\n",
        "        print(ner.text + ' - '+ ner.label_ + ' - ' + \n",
        "               str(spacy.explain(ner.label_)))\n",
        "else:\n",
        "    print(\"No Entity Found\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}